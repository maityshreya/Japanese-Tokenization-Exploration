# Japanese Tokenization Exploration ğŸš€

This repository explores various Japanese tokenization techniques essential for downstream NLP applications like resume-job matching, keyword extraction, and information retrieval.

## ğŸ” Objective

To understand, compare, and document the use of three major Japanese tokenizers:

- **MeCab** â€“ a widely-used morphological analyzer.
- **SudachiPy** â€“ suitable for business/technical Japanese.
- **SentencePiece** â€“ subword tokenizer for ML model input (e.g., BERT, GPT).

## ğŸ§  Why Tokenization?

Unlike English, Japanese text lacks whitespaces between words. Tokenization (word segmentation) is crucial for:

- Accurate keyword extraction
- Matching algorithms (TF-IDF, BM25, etc.)



# Japanese Tokenization Exploration 🚀

This repository explores various Japanese tokenization techniques essential for downstream NLP applications like resume-job matching, keyword extraction, and information retrieval.

## 🔍 Objective

To understand, compare, and document the use of three major Japanese tokenizers:

- **MeCab** – a widely-used morphological analyzer.
- **SudachiPy** – suitable for business/technical Japanese.
- **SentencePiece** – subword tokenizer for ML model input (e.g., BERT, GPT).

## 🧠 Why Tokenization?

Unlike English, Japanese text lacks whitespaces between words. Tokenization (word segmentation) is crucial for:

- Accurate keyword extraction
- Matching algorithms (TF-IDF, BM25, etc.)


